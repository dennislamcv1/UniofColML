{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6963b318ae4bc15d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Grading\n",
    "This week's lab doesn't have any auto-graded components. Each question in this notebook has an accompanying Peer Review question. Although the lab shows as being ungraded, you need to complete the notebook to answer the Peer Review questions. <br>\n",
    "**DO NOT CHANGE VARIABLE OR METHOD SIGNATURES** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ccf74e298d0ee1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Validate Button\n",
    "This week's lab doesn't have any auto-graded components. Each question in this notebook has an accompanying Peer Review question. Although the lab shows as being ungraded, you need to complete the notebook to answer the Peer Review questions. \n",
    "\n",
    "You do not need to use the Validate button for this lab since there are no auto-graded components. If you hit the Validate button, it will time out given the number of visualizations in the notebook. Cells with longer execution times cause the validate button to time out and freeze. ***This notebook's Validate button time-out does not affect the final submission grading.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SVvKTuis8Q_",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd91614c3f1d711b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 2. Stochastic Gradient Descent\n",
    "***\n",
    "\n",
    "In this assignment we'll implement a rudimentary Stochastic Gradient Descent algorithm to learn the weights in simple linear regression.  Then we'll see if we can make it more efficient.  Finally, we'll investigate some graphical strategies for diagnosing convergence and tuning parameters. \n",
    "\n",
    "**Note**: The cell below has some helper functions.  Scroll down and evaluate those before proceeding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest\n",
    "import numpy as np \n",
    "import matplotlib.pylab as plt \n",
    "import pytest\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odKqzgMWs8RE",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-daba0c234c347aa1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mycolors = {\"blue\": \"steelblue\", \"red\":\"#a76c6e\",  \"green\":\"#6a9373\", \"smoke\": \"#f2f2f2\"}\n",
    "\n",
    "def eval_RSS(X, y, b0, b1):\n",
    "    rss = 0 \n",
    "    for ii in range(len(df)):\n",
    "        xi = df.loc[ii, \"x\"]\n",
    "        yi = df.loc[ii, \"y\"]\n",
    "        rss += (yi - (b0 + b1 * xi)) ** 2\n",
    "    return rss\n",
    "\n",
    "def plotsurface(X, y, bhist=None):\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-1, 5, 300))\n",
    "    Z = np.zeros((xx.shape[0], yy.shape[0]))\n",
    "    for ii in range(X.shape[0]):\n",
    "        Z += (y[ii] - xx - yy * X[ii,1]) ** 2\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "    levels = [125, 200] + list(range(400,2000,400))\n",
    "    CS = ax.contour(xx, yy, Z, levels=levels)\n",
    "    ax.clabel(CS, CS.levels, inline=True, fontsize=10)\n",
    "    ax.set_xlim([-3,3])\n",
    "    ax.set_ylim([-1,5])\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=20)\n",
    "    if bhist is not None:\n",
    "        for ii in range(bhist.shape[0]-1):\n",
    "            x0 = bhist[ii][0]\n",
    "            y0 = bhist[ii][1]\n",
    "            x1 = bhist[ii+1][0]\n",
    "            y1 = bhist[ii+1][1]\n",
    "            ax.plot([x0, x1], [y0, y1], color=\"black\", marker=\"o\", lw=1.5, markersize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWXKzcnss8RG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9294db34c313b9f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 1: Setting Up Simulated Data and a Sanity Check \n",
    "***\n",
    "\n",
    "We'll work with simulated data for this exercise where our generative model is given by \n",
    "\n",
    "$$\n",
    "Y = 1 + 2X + \\epsilon \\textrm{ where} \\epsilon \\sim N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "**Part A**: The following function will generate data from the model. We'll grab a training set of size $n=100$ and a validation set of size $n = 50$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-7TZ66Ss8RH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bea36cf28b71f15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "5e8bd759-6e56-47a7-aeca-827620b49c99"
   },
   "outputs": [],
   "source": [
    "def dataGenerator(n, sigsq=1.0, random_state=1236):\n",
    "    np.random.seed(random_state)\n",
    "    x_train = np.linspace(-1, 1, n)\n",
    "    x_valid = np.linspace(-1, 1, int(n / 4))\n",
    "    y_train = 1 + 2 * x_train + np.random.randn(n)\n",
    "    y_valid = 1 + 2 * x_valid + np.random.randn(int(n / 4))\n",
    "    return x_train, x_valid, y_train, y_valid \n",
    "\n",
    "x_train, x_valid, y_train, y_valid = dataGenerator(100)\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.scatter(x_train, y_train, color=\"steelblue\", s=100, label=\"train\")\n",
    "ax.scatter(x_valid, y_valid, color=\"#a76c6e\", s=100, label=\"valid\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_xlabel(\"X\", fontsize=16)\n",
    "ax.set_ylabel(\"Y\", fontsize=16)\n",
    "ax.legend(loc=\"upper left\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_NaHHWbs8RI",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f588d921fed6ea98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B**: Since we're going to be implementing things ourselves, we're going to want to prepend the data matrices with a column of ones so we can fit a bias term.  We can do this using numpy's [column_stack](https://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd6U5Y6Rs8RJ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0396be6a1f3d023c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.column_stack((np.ones_like(x_train), x_train))\n",
    "X_valid = np.column_stack((np.ones_like(x_valid), x_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuTtpY8ds8RJ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30593efb1a4eeea8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part C**: Finally, let's fit a linear regression model with sklearn's LinearRegression class and print the coefficients so we know what we're shooting for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghKk5dM3s8RK",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9a7993d0f230488",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f4752407-5887-491b-a7fd-4e208f64076f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"sklearn says the coefficients are \", reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtLR0q-os8RL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b0e8df7682916022",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part D**: The last thing we'll do is visualize the surface of the RSS, of which we're attempting to find the minimum. Does it looks like the parameters reported by sklearn lie at the bottom of the RSS surface?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iU1KGAv9s8RL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3200808182a5efea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "79ee8c4d-e3a6-4005-d0a0-14c5a64ad00d"
   },
   "outputs": [],
   "source": [
    "plotsurface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eirC_Wuys8RM",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-22faafb25d38f355",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 2: Implementing and Improving SGD \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHpb-eXEs8RN",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec047e8b224ce924",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part A**: Now it's time to implement Stochastic Gradient Descent.  Most of the code in the function sgd has been written for you.  Your job is to fill in the values of the partial derivatives in the appropriate places.  Recall that the update scheme is given by \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 &\\leftarrow& \\beta_0 - \\eta \\cdot 2 \\cdot \\left[(\\beta_0 + \\beta_1x_i) -y_i \\right] \\\\\n",
    "\\beta_1 &\\leftarrow& \\beta_1 - \\eta \\cdot 2 \\cdot \\left[(\\beta_0 + \\beta_1x_i) -y_i \\right] x_i\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Note that the function parameter beta is a numpy array containing the initial guess for the solve. The numpy array bhist stores the approximation of the betas after each iteration for plotting and diagnostic purposes. <br>\n",
    "Look at the Peer Review assignment for a question about this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dy9wnJr9s8RN",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b65f9152be5eab91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sgd(X, y, beta, eta=0.1, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Peform Stochastic Gradient Descent \n",
    "    \n",
    "    :param X: matrix of training features \n",
    "    :param y: vector of training responses \n",
    "    :param beta: initial guess for the parameters\n",
    "    :param eta: the learning rate \n",
    "    :param num_epochs: the number of epochs to run \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize history for plotting \n",
    "    bhist = np.zeros((num_epochs+1, len(beta)))\n",
    "    bhist[0,0], bhist[0,1] = beta[0], beta[1]\n",
    "    \n",
    "    # perform steps for all epochs \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        # shuffle indices (randomly)\n",
    "        shuffled_inds = list(range(X.shape[0]))\n",
    "        np.random.shuffle(shuffled_inds)\n",
    "        \n",
    "        # TODO: loop over training examples, update beta (beta[0] and beta[1]) as per the above formulas\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        # save history \n",
    "        bhist[epoch, :] = beta\n",
    "        \n",
    "    # return bhist. Last row \n",
    "    # are the learned parameters. \n",
    "    return bhist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d5a236c8d93acce8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# SGD Test for 2 features\n",
    "np.random.seed(42)\n",
    "\n",
    "mock_X = np.array([[ 1., -1.], [ 1., -0.97979798], [ 1., -0.95959596], [ 1., -0.93939394]])\n",
    "mock_y = np.array([-1.09375848, -2.65894663, -0.51463485, -2.27442244])\n",
    "mock_beta_start = np.array([-2.0, -1.0])\n",
    "\n",
    "mock_bhist_exp = np.array([[-2., -1.], [-2.01174521, -0.98867152], [-2.02304238, -0.97777761], [-2.03400439, -0.96720934]])\n",
    "mock_bhist_act = sgd(mock_X, mock_y, beta=mock_beta_start, eta=0.0025, num_epochs=3)\n",
    "\n",
    "for exp, act in zip(mock_bhist_exp, mock_bhist_act):\n",
    "    assert pytest.approx(exp, 0.0001) == act, \"Check sgd function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-XIKGP3s8RO",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66e2b113eea67fa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c6bc533d-ab61-4567-9ffd-11024a8cd702"
   },
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "beta_start = np.array([-2.0, -1.0])\n",
    "\n",
    "# Training \n",
    "%time bhist = sgd(X_train, y_train, beta=beta_start, eta=0.0025, num_epochs=1000) # old = 0.0025\n",
    "\n",
    "# Print and Plot \n",
    "print(\"beta_0 = {:.5f}, beta_1 = {:.5f}\".format(bhist[-1][0], bhist[-1][1]))\n",
    "plotsurface(X_train, y_train, bhist=bhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PyfnfIGs8RO",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d240e9f2269de34b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B**: Thinking about the case where we have more than two features, can you think of a way to vectorize the stochastic gradient update of the parameters? When you see it, go back to the sgd function and improve it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeIHWndFs8RO",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f577cdea70d671c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## TODO: rewrite/modify the sgd function below. Do not modify the previous sgd function, but write a new one here. \n",
    "## Do not change the function name.\n",
    "## The previous question worked for 2 features and this function is for more than 2 features so update the earlier \n",
    "# logic to work for any number of features\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-23bc9f9f773e5738",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# SGD Test for more than 2 features\n",
    "np.random.seed(42)\n",
    "\n",
    "mock_X = np.array([[ 1., -1.], [ 1., -0.97979798], [ 1., -0.95959596], [ 1., -0.93939394]])\n",
    "mock_y = np.array([-1.09375848, -2.65894663, -0.51463485, -2.27442244])\n",
    "mock_beta_start = np.array([-2.0, -1.0])\n",
    "\n",
    "mock_bhist_exp = np.array([[-2., -1.], [-2.01174521, -0.98867152], [-2.02304238, -0.97777761], [-2.03400439, -0.96720934]])\n",
    "mock_bhist_act = sgd(mock_X, mock_y, beta=mock_beta_start, eta=0.0025, num_epochs=3)\n",
    "\n",
    "for exp, act in zip(mock_bhist_exp, mock_bhist_act):\n",
    "    assert pytest.approx(exp, 0.0001) == act, \"Check sgd function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhqzdPZVs8RP",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3b156e3785f5a6f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0e550fcf-b2c4-4300-8efd-91bcfbcbb7f8"
   },
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "beta_start = np.array([-2.0, -1.0])\n",
    "\n",
    "# Training \n",
    "%time bhist = sgd(X_train, y_train, beta=beta_start, eta=0.0025, num_epochs=1000)\n",
    "\n",
    "# Print and Plot \n",
    "print(\"beta_0 = {:.5f}, beta_1 = {:.5f}\".format(bhist[-1][0], bhist[-1][1]))\n",
    "plotsurface(X_train, y_train, bhist=bhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7r-9Wy-s8RP",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2fb5221b1d592641",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part C**: Now that you have created this beautiful solver, go back and break it by playing with the learning rate. Does the learning rate have the effect on convergence that you expect when visualized in the surface plot? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3q1TabXs8RQ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79afab5ab3104a37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "874d49d9-0c23-406c-bff7-a5f121e3e692"
   },
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "beta_start = np.array([-2.0, -1.0])\n",
    "\n",
    "# Training\n",
    "# your code here\n",
    "\n",
    "\n",
    "# Print and Plot \n",
    "print(\"beta_0 = {:.5f}, beta_1 = {:.5f}\".format(bhist[-1][0], bhist[-1][1]))\n",
    "plotsurface(X_train, y_train, bhist=bhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcJxcB4bs8RQ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a733d2053b37806",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "3a296494-6153-4c09-d926-bd243cfe19bb"
   },
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6bdVWTNs8RR",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e513e82ad89f7565",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "46f29a4f-1c4d-401c-cdb5-8eefc3a0ba9d"
   },
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkl6kBgvs8RR",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-166a2dbe5d18354d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "d35e0673-7071-4236-a43c-b6d85cab589a"
   },
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3tHsMJOs8RR",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8302eb0b3605ba11",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "19a6f612-ed56-41db-bea8-2a9a12fce85c"
   },
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi1FCblws8RS",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-63dcdc3d767b6bb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 3: Graphical Diagnosis of Convergence \n",
    "***\n",
    "\n",
    "A common way to monitor the convergence of SGD and to tune hyperparameters (like learning rate and regularization strength) is to make a plot of how the loss function evolves during the training process. That is, we plot the value of the loss function periodically and see if it looks like it's reached a minimum, or see if it's jumping around a lot.  Normally we'd record the value of the loss function as we train, but we'll use the beta histories returned by our solver.  Finally, using the MSE instead of the RSS is a popular choice, so we'll do that.  \n",
    "\n",
    "**Part A**: Modify the function below to take in a beta history and a data set and return a vector of MSE values for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23hpIyr8s8RS",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dbb42112cde53ecb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def MSE_hist(X, y, bhist):\n",
    "    mse = np.zeros(bhist.shape[0])\n",
    "    for epoch in range(bhist.shape[0]):\n",
    "        # TODO\n",
    "        # Calculate mse for each epoch using the below formula\n",
    "        # Hint: Use the standard MSE formula and tweak it to incorporate beta histories\n",
    "        # your code here\n",
    "        mse = 0\n",
    "    \n",
    "    return mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-56b0e20606255bfb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# MSE Tests\n",
    "mock_X = np.array([[ 1., -1.], [ 1., -0.97979798], [ 1., -0.95959596], [ 1., -0.93939394]])\n",
    "mock_y = np.array([-1.09375848, -2.65894663, -0.51463485, -2.27442244])\n",
    "mock_bhist = np.array([[-2., -1.], [-2.01174521, -0.98867152], [-2.02304238, -0.97777761], [-2.03400439, -0.96720934]])\n",
    "\n",
    "mock_mse_exp = np.array([1.1110145, 1.0840896, 1.05916951, 1.03590509])\n",
    "mock_mse_act = MSE_hist(mock_X, mock_y, mock_bhist)\n",
    "\n",
    "assert pytest.approx(mock_mse_exp, 0.0001 ) == mock_mse_act, \"Check MSE_hist function\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASMG27nys8RS",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80de4af5cc89fb2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B**: Next we'll take the MSE history that we just computed and plot it vs epoch number. Based on your plot, would you say that your MSE has converged? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXFx5IKRs8RT",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54f8600b15338089",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "be5c7274-1512-43a2-e7b5-0fe7b8385d1b"
   },
   "outputs": [],
   "source": [
    "# plot MSE history vs. epoch number \n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVBjmS-Cs8RT",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef289ccd9250bdbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part C**: Go back up and change the value of the learning rate to bigger and smaller values (you might also have to adjust the max epochs).  Do the different learning rates have the effect on the MSE plots that you would expect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDNwC2QJs8RT",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07f8c685a73c44bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "e3d18ab1-4124-4522-96d5-dd5ef071ab02"
   },
   "outputs": [],
   "source": [
    "# TODO: change the value of the learning rate to bigger and smaller values, consider adjusting max epochs\n",
    "# test plots \n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owhbzvvcs8RT",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2677b1d9ffaca0f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "72d9e54b-288d-4d0b-866d-a166a1dd8742"
   },
   "outputs": [],
   "source": [
    "# continue testing plots for C \n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFNOnBOus8RU",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-668af21a5c69c735",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part D**: Is the MSE on the training data the best thing to look at when deciding if our training algorithm has converged? Plot the train and validation MSE as a function of epochs. Discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pME0tSn7s8RU",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43fddb187c39b5aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "71c87462-61fc-450e-c6f1-83c4017c055e"
   },
   "outputs": [],
   "source": [
    "# plot train and validation MSE as function of epochs\n",
    "# Start at (-2,1)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "name": "Module2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
